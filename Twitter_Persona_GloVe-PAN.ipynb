{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# * File:    Twitter_Persona_GloVe.py\n",
    "# *\n",
    "# * Author1:  Pavan Kumar K N (pavankumar.karkekopp@ucalgary.ca)\n",
    "# * Date:     11th Aug 2019\n",
    "# * Summary of File:\n",
    "# * Explore mbti_1.csv file acquired from https://www.kaggle.com/datasnaek/mbti-type\n",
    "# * Apply state-of-the-art reported publicly\n",
    "# * Build classifier model that is better using machine learning techniques\n",
    "\n",
    "#Just making sure the right environment is running this script\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Read Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert from XML to Twitter MBTI dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import requests \n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePAN(directory_with_xmls): \n",
    "    # Initialize for Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "    directory_with_xmls = Path(directory_with_xmls)\n",
    "    truth_filepath =  directory_with_xmls / \"truth.txt\"\n",
    "    personality_traits_dict = {}\n",
    "    with open(truth_filepath) as truth_file:\n",
    "        for line in truth_file:\n",
    "            (user_id, gender, age_group, extroverted, stable, agreeable, \n",
    "                                         conscientious, openness) =  line.split(':::')\n",
    "            personality_traits_dict[user_id] = [[], float(openness), float(conscientious), \n",
    "                                                float(extroverted), float(agreeable), float(stable)]\n",
    "    \n",
    "    list_posts = []\n",
    "    list_personality = []\n",
    "\n",
    "    for xmlfile in directory_with_xmls.glob('**/*.xml'):\n",
    "        \n",
    "        # create element tree object \n",
    "        tree = ET.parse(xmlfile) \n",
    "\n",
    "        # get root element \n",
    "        root = tree.getroot() \n",
    "\n",
    "        user_id = root.attrib['id']\n",
    "\n",
    "        # create empty list for news tweets \n",
    "        tweets = []\n",
    "        \n",
    "\n",
    "        # iterate news items \n",
    "        for item in root.findall('document'):\n",
    "            tweet_string = item.text\n",
    "\n",
    "            #Removing mentions\n",
    "            tweet_string = tweet_string.replace('@username', '')\n",
    "            #Removing unecessary spaces\n",
    "            \n",
    "            #Removing URL\n",
    "            tweet_string = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", tweet_string)\n",
    "            tweet_string = tweet_string.strip()\n",
    "            tweets.append(tweet_string)\n",
    "            \n",
    "            j=0\n",
    "            for pos in tweets:\n",
    "                pos = re.sub(\"[^a-zA-Z]\", \" \", pos)\n",
    "                pos = re.sub(\" +\", \" \", pos).lower()\n",
    "                pos = \" \".join([lemmatiser.lemmatize(w) for w in pos.split(' ')])\n",
    "                \n",
    "                if pos!= \" \":\n",
    "                    tweets[j] = pos\n",
    "                else:\n",
    "                    tweets[j] = None\n",
    "\n",
    "                j += 1\n",
    "            \n",
    "        tweets = list(filter(None, tweets))\n",
    "        list_posts.append(\" \".join(tweets))\n",
    "        list_personality.append(np.array(personality_traits_dict[user_id][1:]))\n",
    "    return np.array(list_posts), np.array(list_personality)\n",
    "\n",
    "def annotatePersonality(xmlfile):\n",
    "    truth_filepath = Path(xmlfile).parents[0] / \"truth.txt\"\n",
    "    truth_file = open(truth_filepath)\n",
    "    return personality_profile\n",
    "  \n",
    "def savetoCSV(newsitems, filename): \n",
    "  \n",
    "    # specifying the fields for csv file \n",
    "    fields = ['guid', 'title', 'pubDate', 'description', 'link', 'media'] \n",
    "  \n",
    "    # writing to csv file \n",
    "    with open(filename, 'w') as csvfile: \n",
    "  \n",
    "        # creating a csv dict writer object \n",
    "        writer = csv.DictWriter(csvfile, fieldnames = fields) \n",
    "  \n",
    "        # writing headers (field names) \n",
    "        writer.writeheader() \n",
    "  \n",
    "        # writing data rows \n",
    "        writer.writerows(newsitems) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_posts, list_personality = parsePAN(\"data/pan15-author-profiling-training-dataset-2015-04-23/pan15-author-profiling-training-dataset-english-2015-04-23/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = \"data/glove.twitter.27B\"\n",
    "MAX_SEQUENCE_LENGTH = 1300\n",
    "MAX_NB_WORDS = 2000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NB_START_EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "GLOVE_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Read the dataset\n",
    "# mbti_data = pd.read_csv(\"data/mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Print first 5 entries in the dataset\n",
    "# mbti_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_posts = mbti_data.posts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split posts by the delimiter, which is ||| as we can see in row 0 and 4 above\n",
    "# filtered_posts = [p.split(\"|||\") for p in raw_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Example of two tweets from row 0\n",
    "# for i in range(5):\n",
    "#     print(\"Tweet #{}:   {}\".format(i+1,filtered_posts[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_types(row):\n",
    "#     t=row['type']\n",
    "\n",
    "#     I = 0\n",
    "#     N = 0\n",
    "#     T = 0\n",
    "#     J = 0\n",
    "    \n",
    "#     if t[0] == 'I': I = 1\n",
    "#     elif t[0] == 'E': I = 0\n",
    "#     else: print('Could not identify label for I-E')\n",
    "        \n",
    "#     if t[1] == 'N': N = 1\n",
    "#     elif t[1] == 'S': N = 0\n",
    "#     else: print('Could not identify label for N-S')\n",
    "        \n",
    "#     if t[2] == 'T': T = 1\n",
    "#     elif t[2] == 'F': T = 0\n",
    "#     else: print('Could not identify label for T-F')\n",
    "        \n",
    "#     if t[3] == 'J': J = 1\n",
    "#     elif t[3] == 'P': J = 0\n",
    "#     else: print('Could not identify label for J-P')\n",
    "#     return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mbti_data_encoded = mbti_data.join(mbti_data.apply(lambda row: encode_types(row), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mbti_data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function to binarize the types into simple lists instead of pandas.series\n",
    "# personality_binary = {'I':1, 'E':0, 'N':1,'S':0, 'T':1, 'F':0, 'J':1, 'P': 0}\n",
    "# binary_personality = [{1:'I', 0:'E'}, \n",
    "#                       {1:'N', 0:'S'},\n",
    "#                       {1:'T', 0:'F'},\n",
    "#                       {1:'J', 0:'P'}]\n",
    "\n",
    "# def translate_personality(personality):\n",
    "#     # transform mbti to binary vector\n",
    "#     return [personality_binary[l] for l in personality]\n",
    "\n",
    "\n",
    "# def translate_binary(personality):\n",
    "#     # transform binary vector to mbti personality\n",
    "#     s = \"\"\n",
    "#     for i, l in enumerate(personality):\n",
    "#         s += binary_personality[i][l]\n",
    "#     return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = mbti_data_encoded.head(4)\n",
    "# list_personality_bin = np.array([translate_personality(p) for p in mbti_data_encoded.type])\n",
    "# print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mbti_data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #List of strings to remove from the corpus\n",
    "# unique_type_list = ['INFJ', \n",
    "#                     'ENTP', \n",
    "#                     'INTP', \n",
    "#                     'INTJ', \n",
    "#                     'ENTJ', \n",
    "#                     'ENFJ', \n",
    "#                     'INFP', \n",
    "#                     'ENFP',\n",
    "#                     'ISFP', \n",
    "#                     'ISTP', \n",
    "#                     'ISFJ', \n",
    "#                     'ISTJ', \n",
    "#                     'ESTP', \n",
    "#                     'ESFP', \n",
    "#                     'ESTJ', \n",
    "#                     'ESFJ']\n",
    "\n",
    "# unique_type_list = [x.lower() for x in unique_type_list]\n",
    "\n",
    "# # Initialize for Lemmatization\n",
    "# stemmer = PorterStemmer()\n",
    "# lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# def pre_process_data(data, remove_stop_words = True, remove_mbti_profiles = True):\n",
    "#     list_personality = []\n",
    "#     list_posts = []\n",
    "#     len_data = len(data)\n",
    "#     i=0\n",
    "    \n",
    "#     for row in data.iterrows():\n",
    "#         i+=1\n",
    "#         if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "#             print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "#         ##### Remove and clean comments\n",
    "#         posts = row[1].posts\n",
    "#         filtered_post = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", posts)\n",
    "#         filtered_post_list = filtered_post.split(\"|||\")\n",
    "#         j=0\n",
    "#         for pos in filtered_post_list:\n",
    "#             pos = re.sub(\"[^a-zA-Z]\", \" \", pos)\n",
    "#             pos = re.sub(\" +\", \" \", pos).lower()\n",
    "#             if remove_stop_words:\n",
    "#                 pos = \" \".join([lemmatiser.lemmatize(w) for w in pos.split(' ') if w not in stopwords.words(\"english\")])\n",
    "#             else:\n",
    "#                 pos = \" \".join([lemmatiser.lemmatize(w) for w in pos.split(' ')])\n",
    "\n",
    "#             #Removing occurrances of MBTI profile strings in tweets\n",
    "#             if remove_mbti_profiles:\n",
    "#                 for t in unique_type_list:\n",
    "#                     pos = pos.replace(t, \"\")\n",
    "                    \n",
    "#             if pos!= \" \":\n",
    "#                 filtered_post_list[j] = pos\n",
    "#             else:\n",
    "#                 filtered_post_list[j] = None\n",
    "    \n",
    "#             j += 1\n",
    "\n",
    "            \n",
    "# #             print(\"Tweet #{}:   {}\".format(j+1, pos))\n",
    "\n",
    "#         filtered_post_list = list(filter(None, filtered_post_list))\n",
    "#         type_labelized = translate_personality(row[1].type)\n",
    "#         list_personality.append(type_labelized)\n",
    "#         list_posts.append(\" \".join(filtered_post_list))\n",
    "# #         print(filtered_post_list)\n",
    "\n",
    "#     list_posts = np.array(list_posts)\n",
    "#     list_personality = np.array(list_personality)\n",
    "#     return list_posts, list_personality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_posts, list_personality  = pre_process_data(mbti_data_encoded, remove_stop_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('thing i want for my business card but are too expensive pm color colored edge soft touch finish raised spot uv cut corner   painter produced their most highly valued work when they were year old so i ve year left for mine  your new discussion layout is confusing regarding who say what because the comment aren t sectioned off  i never really understood why game environment which you see most of get so few resource compared to character   k and on a gun fine but throwing the same on an enemy that you try to shoot before he get too close  gun using the texture already in memory is good gun on the floor having the same model a fps isn t probably lod model  i m talking about char that are small scale compared to enviro prop extreme example tri scene dammit crendor you broke jesse s game and made the audio go poof totally your fault  i liken esther to a movie you can choose to progress or not but not affect much else but really grey area  gamers unfazed sale ha arrived the tiniest price finest of prize gaben be praised discount shall thrive  indie gamedev i don t want to be bound by publisher and marketing test indy gamedev game are an artform they belong in a museum   mbit here get for in the netherlands previous contract since or wa same speed  on the flipside half game for half price  happy new year with an appropriately time new site  just played the first two episode of quake again game s still tight loving the smooth movement  so apparently paying a month for spotify make me worse than a music pirate dafuq  wanted to say i appreciate the choice for minion should make the game feel nicely crowded and give bad player more fun  heh i think it s funny you re apologizing for sound quality that s still better than say of youtube game voiceovers  i know this is cool and serious science but it had me giggling like a kid best of both world  working on refining my logo to chamfer or nor to chamfer that is the question   c to protect and infect youtube i don t know a whole lot about network security but enough that this scare me  silly spotify white zombie judge dredd soundtrack rob zombie blue man group okay then  thanks for mentioning the hearthstone open beta i ve been anxiously waiting but hadn t gotten any email about it yet  do you perchance read jezebel the timing is rather close  i am so sick of people on twitter complaining about a bunch people on twitter   help to have a map no no it doesn t  remember the time you paid grand for celebrity fashion without airbrushing pic so do these people  steam s down wonder if that s because of the new music stuff  play this now tomorrow during lunch whenever it s only minute but it s awesome  first time i bought a key for dota treasure chest baby roshan pretty sweet deal  hah that shit s golden telltale  are you still looking for people i can free up some time and be almost completely dedicated the next month s  i threw up some isometric example at assuming you want to stick with the rpgmaker perspective  not many developer company would opt out of lot of money because something s addictive i respect you for that  now that s a proper free to play dungeon keeper  i like this  how do they not help i find it very useful to know if a game ha been designed for touchscreen controller or kbm  remember that fake twitch lol video from this afternoon here s a real twitch play dota channel  thanks for reminding me u to disable adblock for place i regularly visit because it s often a fairer trade that tv radio  mini metro quite a novel idea build your own growing metro system not sure about longevity replayability though  how about keeping the same root word and call them bullvids  coil because of it several experiment mirror s edge because of it absence and whichever game started radial menu crysis  consider that a free game thrown in by the god for your generous purchase   luft mean air not red and rauser probably stem from the dutch verb rouzen playing wildly or razen rage storm  did you guy just change all your youtube video thumbnail or have i not paid proper attention lately  do you also know about  is this april fool or did they really start a website on april st with the plan of confusing people on every anniversary  whelp wanted drama he got drama can the devs get a nice game jam now since that s what they wanted game jam great april fool joke anita sarkeesian receiving gdc ambassador award for making video in two year time  for a moment i thought i wa reading about a game from your avvy is now black red white doe this mean future content will be in betrayer vision  hey i wanted to inform you that potato dungeon seems to be borked right now clicking to ride pard doesn t work for me anyways  not fond of new design discrover show le per page and the menu bar show le line so i must scroll to get to my playlist  it s a carefully planned deal he asked both for the exact same amount so he s raking in cash but staying neutral  combine any random music term in youtube it probably ha it raprock acapella metal industrial jazz celtic chiptunes gregorian dubstep  i feel like i can express myself le right now it s all white will there be a way to change style besides just link color  harold and kumar walk down the aisle because racial inclusivity of course  just crashed in to more bug in minute on the bike than in the past two week of game development  goddamnit i hate it when a commenting system force you to first register but meanwhile doesn t save your typed post lookingatu is the new ui broken or just badly designed i can no longer sort album or search query by name artist album length rating  yes but only cool people  might be cool for portfolio horizontal scrolling single page template also on github html c  i wonder how many people realized what horrible thing they sometimes say good luck best wish and dump that lump  just rode past some kid biking km for some i guess that s pretty healthy in the end  if you sit like this and stare past your crotch it probably is but only barely  here s hoping this will wake people up a bit about greenlight early access kickstarter right  what you re saying is you re now opening a p o box under the name of cynicalcox  you hypocrite you closing down your p o box for other company but still accepting bribe from polaris bias bias  yay mustasch  jim is in fact a drag king in real life he is an ethereal spirit but for his show he dress up a a man  the point is not better because harder but balanced against lame exploit it mean you actually think engage with the game  this should be enough energydrink for at least two day  bad pixel shirt are one of those thing that annoy and bother me more than they probably should  so what do we call this kind of review score hate strife out of ten  good old gaming a store kinda like steam origin but aimed at retro indie more than aaa  u s healthcare it ll cost you an arm and a leg either way  get the spambots under control please  and here s yet another one  more spammer clean them up  no but i can reccommend you a great shirt  turn out you should not attempt to run km barefoot when you haven t done so in three year ouch  whoa whoa whoa not just a black baseball cap one with a very iconic stitchy liny logo thing  half life no no carmack made doom quake rage half life is from  why is there something wrong with them are the ad too silent to hear for you we are not having that issue  going to try and get something done for lowrezjam day left so it s gonna be tight but i know what i m gonna make a x fighter  i m not seeing the problem with that lowrezjam workfile look perfectly usable  a long a everything is in nice sprite strip i m happy for small stuff like this panning around is easier than tab window  face pulp actually i should probably have scaled up the image p heck i m working small enough to make the game  i d like to know which devs you consider upstanding trustworthy what with nintendo greed watchdog graphic and  indie such a town and that recent plane game air control who do you like still  i just spent an absurd amount of time playing with this remarkable online sid keyboard emulator  while your point is true you re not the best advocate lot of woman feature their look notallwomen the time when your dog want to eat the rotting half corpse of a mouse and you have to pluck it out of her mouth  hey i wanted to check if you were aware of this pixel pig game using your artwork  good vid but missing one element presentation host a much content a possible have curated list of the best the new   the interesting and so on make it easier to navigate empowering critic user not curation but promotion navigation  wow this wa great i love simple game like these that require very little time complexity to convey a strong idea  oh no is offline a are the forum sound the alarm sound the alarm  the concentric pattern make me wonder if dithering could make the layer transition more natural looking  with facebook in your future doe this mean we might finally get a legitimate desktop version of whatsapp ',\n",
       " array([0.5, 0.1, 0.2, 0.1, 0.2]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_posts[0], list_personality[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Matrix with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove.twitter.27B.' + str(GLOVE_DIM) + 'd.txt'\n",
    "emb_dict = {}\n",
    "glove = open(os.path.join(GLOVE_DIR, glove_file), encoding=\"utf-8\")\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the word airplane in the dictionary:[ 0.24663    0.15368    0.66699   -0.47329    0.48851    0.56863\n",
      " -0.27034   -0.56281    0.47398   -0.59753   -0.019399   0.20907\n",
      " -1.9358    -0.039432   0.006689  -0.20946    0.44273    0.87527\n",
      " -0.010597  -0.27101   -1.3333    -0.66836    0.45542   -0.2994\n",
      "  0.91341    0.87099   -0.45495    0.093829   0.31513    0.56531\n",
      "  0.24558   -0.54483    0.32325   -0.55472    0.038418   0.40444\n",
      "  0.22255    0.27899    0.25924    0.023309   0.46968    0.62029\n",
      " -0.17067   -0.70975    0.79782   -0.17841    0.36865   -0.0076834\n",
      " -0.35966    0.086881   0.31657    0.91939   -0.20271   -0.18098\n",
      " -0.31798    0.41899    0.1277    -0.5368     0.27473   -0.12952\n",
      "  0.9968     0.27832    0.37444    0.066832  -1.0672    -0.65536\n",
      " -0.27039   -0.11158    0.16106   -0.02075   -0.24849    0.34627\n",
      " -0.27947   -0.76005   -0.087761  -0.0748    -0.76778   -0.25303\n",
      "  1.2236    -0.12425    1.2025     0.41603    0.037124   0.1475\n",
      "  0.29442   -0.6001     0.01312   -0.40055   -1.09       0.022745\n",
      "  0.53056   -0.36269   -0.71336    0.11547   -0.50841   -0.26991\n",
      " -0.28229   -0.24338   -0.78637    0.35897  ]\n",
      "Found the word airline in the dictionary:[-0.32545    0.063732   0.70078   -0.63015   -0.14481   -0.48925\n",
      " -0.56835    0.1076     0.82454   -0.7904     0.27804    0.18418\n",
      " -1.6156     0.58212    0.80614   -0.2736     0.75243    0.59432\n",
      "  0.037589  -0.5391    -1.0012    -0.60133    0.59707   -0.15888\n",
      " -0.10687    0.94695    0.25258    0.21075    0.26468    0.1085\n",
      "  0.3446    -0.50101   -0.41914   -0.1632     0.22788   -0.79264\n",
      "  0.75174    1.1037    -0.18882   -0.25413    0.30063    0.55704\n",
      "  0.040197  -0.50817    0.53364    0.60152    0.10418   -0.94543\n",
      "  0.85121    0.14108    0.78171    0.61566   -1.2194     0.51622\n",
      " -0.85656    0.70914    0.058446  -0.27939   -0.43167   -0.19958\n",
      "  0.45471    0.35038    0.30411    0.67821    0.0076719 -0.02914\n",
      " -0.57676   -0.21682   -0.34525   -0.070283  -0.65043    0.35322\n",
      "  0.3482    -0.36953    0.29752   -0.43913   -0.70563   -0.401\n",
      "  0.65551   -0.54257    1.1211     0.07359    0.34736    0.96744\n",
      "  0.055915  -0.0090149 -0.34113    0.039693  -0.60119   -0.63442\n",
      " -0.22582    0.2934    -0.48509    0.53609   -0.48478   -0.277\n",
      " -0.7002     0.22019   -0.8303     0.44253  ]\n",
      "Found the word flight in the dictionary:[ 2.4154e-01  5.2802e-01  2.1040e-01 -8.4436e-01  3.5335e-01 -3.4378e-01\n",
      "  1.1245e-01  1.0319e-01  1.5893e-01  2.8021e-02  5.7265e-01  1.5235e-01\n",
      " -2.9205e+00  3.8788e-01  6.1903e-01 -5.5681e-02  4.7579e-01  4.7197e-01\n",
      " -2.7068e-01 -6.0496e-01 -1.1766e+00 -1.0527e+00  4.2055e-01 -3.8046e-01\n",
      "  4.9691e-01  1.1100e-01 -2.5902e-01 -7.5999e-02 -4.1112e-01  2.4493e-01\n",
      " -4.4429e-01 -4.4753e-02 -1.6889e-01 -1.2417e-01  3.0427e-01  3.8993e-01\n",
      "  2.2299e-01  3.1317e-01  4.5619e-02  1.9553e-01 -2.8174e-01  7.0545e-01\n",
      "  3.9683e-01 -3.0806e-01  9.4105e-01  3.9789e-01  6.4337e-01 -1.1392e+00\n",
      "  4.6979e-01  7.1418e-01  1.9361e-01  1.3685e-01 -2.2736e-01  3.5347e-01\n",
      " -2.4625e-01  2.6179e-01  4.6017e-01 -3.6056e-01  1.1724e-01 -1.2602e-01\n",
      "  1.0881e+00  3.1204e-01  5.3272e-01  4.1870e-01 -4.8034e-01  1.1047e-01\n",
      " -8.1975e-01 -1.0848e+00 -2.8562e-03 -2.3981e-01 -1.8529e-01  3.0807e-01\n",
      " -2.3230e-01 -3.0965e-01  1.0894e+00 -3.5962e-01 -2.1058e-01  6.2088e-02\n",
      "  8.3815e-01 -5.4961e-01  1.5611e+00 -3.1935e-01  5.8810e-01  8.9088e-01\n",
      "  3.5812e-01 -2.9284e-01  4.6158e-02  2.0048e-02  5.9720e-01  5.3041e-02\n",
      "  1.5525e-01  4.8072e-01 -4.3539e-01  3.9914e-01 -3.4885e-01 -4.1527e-01\n",
      " -2.0266e-01  5.3678e-01 -6.4956e-01 -2.5494e-01]\n",
      "Found the word luggage in the dictionary:[ 0.38576   -0.55682    1.5003    -0.82694    0.7586    -0.0046742\n",
      " -0.35869    0.65129    0.83517   -0.91132    0.41173    0.33397\n",
      " -1.9384     0.033561   0.26024    0.098997   0.27261    0.63064\n",
      "  0.53685    0.057916  -1.1323    -1.109      0.97308    0.53437\n",
      " -0.40052    0.76776    0.25361    1.2392    -0.32863    0.092056\n",
      " -0.40047    0.53313    0.55841   -0.76486    0.33743    0.46378\n",
      " -0.18408    1.0075    -0.29599    0.2435     0.10955    0.070725\n",
      " -0.36677    0.26398    0.50326   -0.3911     0.39558   -1.0475\n",
      " -0.070682   0.83247   -0.17646    0.35337   -0.99499   -0.65134\n",
      " -0.55994    0.04921    0.91603    0.19839   -0.39124    1.0443\n",
      "  0.91908    1.154      0.36598    0.26294   -0.37361    0.16926\n",
      "  0.0081164 -0.55617    0.46883    0.19409   -0.33189    0.44202\n",
      "  0.092389  -0.61699    0.3232    -0.40594   -0.6449    -0.57782\n",
      "  0.5269    -0.67604    0.92799    0.35353   -0.55223   -0.71891\n",
      " -0.24297    0.78749    0.46515   -0.578      0.24205    0.098336\n",
      "  0.069553   0.23499    0.45195    0.02634   -0.55002    0.66194\n",
      " -0.63691    0.3597    -0.20163    0.77162  ]\n"
     ]
    }
   ],
   "source": [
    "airline_words = ['airplane', 'airline', 'flight', 'luggage']\n",
    "for w in airline_words:\n",
    "    if w in emb_dict.keys():\n",
    "        print('Found the word {} in the dictionary:{}'.format(w,emb_dict[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(list_posts, list_personality, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seeing some people giving highest importance to love i really feel surprised love is the last thing in my  all girl are same they try so much to show off that they care that at the end of the day it is more a pain than anything else  just heard a superhit song i want to hit somebody what lyric even better than zandu balm thanks to the autowala  i wish everybody have more reason to live i have mine  all right for a change let s exchange some gyan what is meant by boga   nd day of day rest gt time to understand the capability of my mobile after month of buying nice to  a friendly suggestion to a friend please check what you are posting or people will have wrong interpretation  completely unfair srk in kbc and msd in mohali at a crucial point both should take place seperately i  well that is it i declare end of this war i just couldn t figure out who are my frnds and who are nt  a i declared the end of war more attack statrted coming i am not weak so be careful and she said of love everyday  hillarious good bye fb that s it i guess no more misunderstanding  i am sorry if my post hv hurt anyone i seriously didnt post anythin against anyone except on purpose  i personally apologise to two guy they ll knw you were never my target  life ha become so complicated i wish i could get my smile back dont remember last time  thanks dear  getting lost alone in the crowd uff i am tired of myself  just a long a you stay stand by me  today is waiting for  never tell your problem to anyone don t care and the other are glad you have them  difference between mutual friend and common friend anyone watch out facebook  strike while the iron is hot  rt bhupen hazarika s condition worsens he ha been put on dialysis a both kidney s have failed  live i league like it  you are most dangerous when you don t have any goal for the first time i m afraid of me  all is such bullshit this is simply so wrong  if i am all alone i am lucky i don t have anyone to die for  baby baby baby ohhhh like baby baby baby noooo like baby baby baby ohhhhh i thought you d always be mine it take a minute to have a crush on someone an hour to like someone and a day to love someone but it take  the person who said all is fair in love and war obviously wasn t the person being hurt  rt sachin tendulkar completes test run ibnliveforipad  d p o and i thot it is my past  there must be an end and perhaps this is it   good score where doe it end  had an extensive shopping with a very special person hour gt enuf of xp to upgrade to xp vista  i dont say sorry if i dont mean it  on a shopping spree day in a row now my leg is hurting gathered xp btw  rescheduling life well in a way  hv come to see rockstar hope it turn out good otherwise someone s going to blame me fr it  disappointed evn d best ranbir so far and rocking arr failed to lift d movie which cud hv been far better  nargis wa d weakest point hope imtiaz doe nt go out of heroine so soon  enjoying a movie also depends on the surroundings today it wa the worst  love is a lost art so true  morale of today look before you watch p forget movie somebody is feeling like gopal misra best wish  happy child s day dont let the child within you die stop war make love  it s getting hot fr those missed out gopal misra he is d protagonist of revolution  mask is more for protection than aggression dont push me to be aggresive cuz i m good at dat a well  i am tired people must knw wht they say  rt ind wi nd test rahul dravid score his th test century ibnliveforipad on a lighter mood what should be the ringtone when your loved one call my fav is ye tumhari meri baatein from rock on  wish i had enuf money frm corruption ofcrs i ud hv opened a engg college by now any sponsor  if you hv money no can touch you cheer to corruption  when night ha come and land is dark and the moon is the only light  heard a fish vendor talking to a customer he wa requesting not to use plastic bag nt fr his own profit bt  tired of hearing good thing about me plz point out my bad part a well plz comment  finally achieving at least a single goal set long ago by me hope i maintain it properly  listening to desi boyz like them actually for a change  a simple question i ask my frnds everyday today i want to hear that  this is being rather a long wait  and thankfully the wait is over  finally able to set a goal in life now how to get that trust me c s a buti  i think u r getting terribly wrong  returning to the regular life in few hour tired a much a possible many wonderful picture wait to be shared but i cannot anymore  i posted a new photo to facebook ishrat jahan encounter wa fake sit tell gujarat hc india news ibnlive via a simple question i ask my frnds everyday today i want to hear that and i post it for the second time life is not so simple silly  a year all set to finish a it started well may be both good and bad  heart is a pumping machine right then why doe it pain when one miss someone close  don t feel to share anything when i have so much to tell someday i may kill myself if not someone else  sheer madness surrounding me live and let live must be the first rule  r ashwin wkt and now a century atleast now we should for get about bhajji  it feel weird good mainly when someone i dont know system generated tell u wht i wanted to hear for so long  thanks twitter  alert bitten by the k bug liked hearing the song u song u waiting for the original u  the unmentionable thing is in the air not bad really   hour of sleep for the th day yaaaaaawn feeling super sleepy   hueva dis tym don say no a very close person to me like it is that easy  a girl look best on her marriage day true best dress glittering gold and a shy smile having the best  now i know how it feel i need to apologize but dont know if i ll ever get it  btw it is  the last month of the year plenty to look back and think  if your world is centered around a thing or a person you start making a circle and dont go anywhere   th tweet not a bad going life seldom give you second chance so grab it before you miss it  point of no return ahead alert  taking extra care of myself not bad to start late than never thanks must go to someone starting with p  rt india ha many corrupt shop who refuse to give receipt on cash payment i always insist for it you also should  in difficult time thank all of them who make your life miserable you could not have a better way of making yourself strong  again am stretched morning then workout feeling tired   second well that s pretty much  rt i say it again i never saw sir viv bat but iv seen sehwag bat what a player in a one day game is next to impossible  rt cheer lala congratulation way to go buddy  one of the darkest day of my life a year on  rt every time i see incident like amri i m convinced we really are a rd world nation with delusion of greatness  rt of course the doctor staff fled what else would explain the ratio of staff patient casualty amri  leave the past behind and move on for a better future  everything is falling apart all right life is not so simple dude  two strange thing i saw today keeping pen in the sleeve why wear a watch if it is min fast than original time  a beautiful song marred by a violent co passenger impossible why woman dont forget that it is not is home   n half hour of sleep today it is going to be a hell of a day '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0. , 0.2, 0.2, 0.4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk =  Tokenizer(num_words=MAX_NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(list_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = []\n",
    "for tweet in X_train:\n",
    "    seq_len.append(len(tweet.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     121.000000\n",
       "mean     1097.280992\n",
       "std       333.919157\n",
       "min       316.000000\n",
       "25%       933.000000\n",
       "50%      1091.000000\n",
       "75%      1285.000000\n",
       "max      1806.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(seq_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1300 #based on above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros((MAX_NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    # The word_index contains a token for all words of the training data so we need to limit that\n",
    "    if i < MAX_NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        # Check if the word from the training data occurs in the GloVe word embeddings\n",
    "        # Otherwise the vector is kept with only zeros\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 399, 524, 144])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_trunc[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Splitting Vallidation data\n",
    "# X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "def eval_metric(history, metric_name):\n",
    "    '''\n",
    "    Function to evaluate a trained model on a chosen metric. \n",
    "    Training and validation metric are plotted in a\n",
    "    line chart for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        history : model training history\n",
    "        metric_name : loss or accuracy\n",
    "    Output:\n",
    "        line chart with epochs of x-axis and metric on\n",
    "        y-axis\n",
    "    '''\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
    "    '''\n",
    "    Function to test the model on new data after training it\n",
    "    on the full training data with the optimal number of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        model : trained model\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_test : test features\n",
    "        y_test : test target\n",
    "        epochs : optimal number of epochs\n",
    "    Output:\n",
    "        test accuracy and test loss\n",
    "    '''\n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1300, 8)           16000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10400)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 52005     \n",
      "=================================================================\n",
      "Total params: 68,005\n",
      "Trainable params: 68,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model = models.Sequential()\n",
    "emb_model.add(layers.Embedding(MAX_NB_WORDS, 8, input_length=MAX_SEQUENCE_LENGTH))\n",
    "emb_model.add(layers.Flatten())\n",
    "emb_model.add(layers.Dense(5, activation='softmax'))\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-6125e99a1df0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memb_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeep_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_emb' is not defined"
     ]
    }
   ],
   "source": [
    "emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(emb_history, 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(emb_history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_results = test_model(emb_model, X_train_seq_trunc, y_train, X_test_seq_trunc, y_test, 20)\n",
    "print('/n')\n",
    "print('Test accuracy of word embeddings model: {0:.2f}%'.format(emb_results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embedding with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros((MAX_NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    # The word_index contains a token for all words of the training data so we need to limit that\n",
    "    if i < MAX_NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        # Check if the word from the training data occurs in the GloVe word embeddings\n",
    "        # Otherwise the vector is kept with only zeros\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1300, 100)         200000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 130000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 650005    \n",
      "=================================================================\n",
      "Total params: 850,005\n",
      "Trainable params: 850,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "glove_model = models.Sequential()\n",
    "glove_model.add(layers.Embedding(MAX_NB_WORDS, GLOVE_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "glove_model.add(layers.Flatten())\n",
    "glove_model.add(layers.Dense(5, activation='softmax'))\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-249567f75130>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mglove_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeep_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_emb' is not defined"
     ]
    }
   ],
   "source": [
    "glove_history = deep_model(glove_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-001e42078afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_history' is not defined"
     ]
    }
   ],
   "source": [
    "eval_metric(glove_history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-56112b1936e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'acc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_history' is not defined"
     ]
    }
   ],
   "source": [
    "eval_metric(glove_history, 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-354346a36699>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mglove_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_seq_trunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_seq_trunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy of word glove model: {0:.2f}%'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-ba8c237f2586>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model, X_train, y_train, X_test, y_test, epoch_stop)\u001b[0m\n\u001b[0;32m     69\u001b[0m               \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch_stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m               \u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m               , verbose=0)\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m                 raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[0;32m    682\u001b[0m                                    \u001b[1;34m'training/testing. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m                                    'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "glove_results = test_model(glove_model, X_train_seq_trunc, y_train, X_test_seq_trunc, y_test, 20)\n",
    "print('/n')\n",
    "print('Test accuracy of word glove model: {0:.2f}%'.format(glove_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openness\n",
      "[0.4 0.1 0.4 0.2 0.1 0.1 0.4 0.1 0.2 0.1 0.5 0.3 0.1 0.4 0.3 0.5 0.1 0.2\n",
      " 0.3 0.2 0.1 0.3 0.1 0.2 0.1 0.1 0.3 0.1 0.2 0.2 0.1]\n",
      "Conscientiousness\n",
      "[ 0.2  0.1  0.2  0.1  0.1  0.   0.2  0.   0.3  0.1  0.2  0.   0.3  0.\n",
      "  0.3 -0.2  0.5  0.1  0.3  0.1  0.3  0.   0.1  0.   0.1  0.5  0.4  0.1\n",
      "  0.2  0.   0. ]\n",
      "Exraversion\n",
      "[ 0.4  0.1  0.4  0.1  0.2  0.1  0.1  0.2  0.  -0.1  0.  -0.1  0.2  0.2\n",
      "  0.2  0.1  0.2  0.1  0.1  0.3  0.2  0.3  0.1  0.1  0.2  0.1  0.3  0.5\n",
      " -0.2  0.2  0.1]\n",
      "Agreeableness\n",
      "[ 0.1  0.1  0.1  0.   0.2  0.2  0.3  0.2 -0.3  0.1  0.   0.2  0.3  0.\n",
      "  0.1  0.2 -0.1  0.   0.   0.2  0.1  0.4 -0.1  0.2  0.2 -0.1  0.3  0.1\n",
      "  0.1 -0.2  0.2]\n",
      "Neuroticism\n",
      "[ 0.5  0.1  0.5  0.4  0.4  0.2  0.3 -0.1 -0.2 -0.1 -0.3  0.3  0.1  0.5\n",
      "  0.2 -0.3 -0.1  0.4  0.2  0.3  0.2  0.1 -0.1 -0.1  0.1  0.1  0.5  0.1\n",
      "  0.4  0.1  0.2]\n"
     ]
    }
   ],
   "source": [
    "type_indicators = [ \"Openness\", \"Conscientiousness\", \"Exraversion\", \"Agreeableness\", \"Neuroticism\"]\n",
    "\n",
    "for l in range(len(type_indicators)):\n",
    "    print(type_indicators[l])\n",
    "    print(y_test[:,l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Openness ...\n",
      "(121, 1300) (121,)\n",
      " RMSE: 0.188 MAE: 0.148\n",
      "\n",
      "\n",
      "Conscientiousness ...\n",
      "(121, 1300) (121,)\n",
      " RMSE: 0.180 MAE: 0.129\n",
      "\n",
      "\n",
      "Exraversion ...\n",
      "(121, 1300) (121,)\n",
      " RMSE: 0.198 MAE: 0.158\n",
      "\n",
      "\n",
      "Agreeableness ...\n",
      "(121, 1300) (121,)\n",
      " RMSE: 0.197 MAE: 0.142\n",
      "\n",
      "\n",
      "Neuroticism ...\n",
      "(121, 1300) (121,)\n",
      " RMSE: 0.283 MAE: 0.216\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(type_indicators)):\n",
    "    print(\"\\n\\n{} ...\".format(type_indicators[l]))\n",
    "    \n",
    "    y_train_class = y_train[:,l]\n",
    "    y_test_class = y_test[:,l]\n",
    "    \n",
    "    print(X_train_seq_trunc.shape, y_train_class.shape)\n",
    "    seed = 7    \n",
    "    model = XGBClassifier(learning_rate=0.01,\n",
    "                             n_estimators=5000,\n",
    "                             max_depth=4,\n",
    "                             min_child_weight=6,\n",
    "                             colsample_bytree=0.8,\n",
    "                             objective='binary:logistic',\n",
    "                             nthread=8,\n",
    "                             scale_pos_weight=1,\n",
    "                             seed=7)\n",
    "\n",
    "    model.fit(X_train_seq_trunc, y_train_class)\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test_seq_trunc)\n",
    "    predictions = [value for value in y_pred]\n",
    "    def rmse(y, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred - y)**2))\n",
    "    rmse_scorer = sklearn.metrics.make_scorer(rmse, greater_is_better=False)\n",
    "    scores = cross_validate(model, X_train_seq_trunc, y_train_class, cv=10, scoring=rmse_scorer)\n",
    "\n",
    "\n",
    "    # evaluate predictions\n",
    "#     accuracy = sklearn.metrics.accuracy_score(y_test, predictions)\n",
    "#     f1_score_measure = sklearn.metrics.f1_score(y_test, predictions)\n",
    "    rmse_val = rmse(y_test_class, predictions)\n",
    "    \n",
    "#     # evaluate predictions\n",
    "#     accuracy = sklearn.metrics.accuracy_score(y_test_class, predictions)\n",
    "#     f1_score_measure = sklearn.metrics.f1_score(y_test_class, predictions)\n",
    "    mae = sklearn.metrics.mean_absolute_error(y_test_class, predictions)\n",
    "    print(\" RMSE: {:.3f} MAE: {:.3f}\".format(rmse_val, mae))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Openness ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE: 0.156 MAE: 0.126\n",
      "Cross validation scores:  {'fit_time': array([0.08676767, 0.08976054, 0.09075785, 0.09973264, 0.09075761,\n",
      "       0.09075856, 0.0897603 , 0.08876109, 0.08876419, 0.08976054]), 'score_time': array([0.        , 0.        , 0.        , 0.0009973 , 0.        ,\n",
      "       0.00099707, 0.        , 0.00099754, 0.        , 0.0009973 ]), 'test_score': array([-0.15781382, -0.13024003, -0.2071166 , -0.15877893, -0.21905065,\n",
      "       -0.14889694, -0.17169944, -0.18516357, -0.13308232, -0.20120479]), 'train_score': array([-0.00010113, -0.00010888, -0.00010712, -0.00010849, -0.0001032 ,\n",
      "       -0.00011352, -0.00010183, -0.00011304, -0.0001058 , -0.00012274])}\n",
      "\n",
      "\n",
      "Conscientiousness ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE: 0.163 MAE: 0.129\n",
      "Cross validation scores:  {'fit_time': array([0.08577156, 0.08975983, 0.09075689, 0.09075689, 0.08975959,\n",
      "       0.09075737, 0.0897603 , 0.09075737, 0.08776498, 0.09075665]), 'score_time': array([0.00099778, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.00099707]), 'test_score': array([-0.20038425, -0.1799212 , -0.15346384, -0.1437414 , -0.20536423,\n",
      "       -0.13920169, -0.19285344, -0.1386437 , -0.12944837, -0.18887386]), 'train_score': array([-1.18920680e-04, -1.03167965e-04, -1.10478190e-04, -9.54106512e-05,\n",
      "       -1.14535969e-04, -1.25997700e-04, -9.48457947e-05, -1.09759019e-04,\n",
      "       -1.11485715e-04, -1.10000966e-04])}\n",
      "\n",
      "\n",
      "Exraversion ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE: 0.170 MAE: 0.118\n",
      "Cross validation scores:  {'fit_time': array([0.08577037, 0.08976007, 0.09075689, 0.09374928, 0.09075761,\n",
      "       0.09474683, 0.08976054, 0.09075689, 0.0897603 , 0.08876252]), 'score_time': array([0.00099754, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ]), 'test_score': array([-0.16840315, -0.19621188, -0.1997994 , -0.21790773, -0.22636299,\n",
      "       -0.15753066, -0.21917288, -0.24858312, -0.17899641, -0.14844257]), 'train_score': array([-0.00012241, -0.00011035, -0.00011127, -0.00011954, -0.00010423,\n",
      "       -0.00010721, -0.0001157 , -0.00012213, -0.00010934, -0.0001077 ])}\n",
      "\n",
      "\n",
      "Agreeableness ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE: 0.195 MAE: 0.144\n",
      "Cross validation scores:  {'fit_time': array([0.08676815, 0.09175515, 0.09075665, 0.09075642, 0.09574223,\n",
      "       0.09374952, 0.1017282 , 0.08976054, 0.08776498, 0.08876276]), 'score_time': array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.0009973 , 0.        , 0.00099754, 0.        ]), 'test_score': array([-0.16879733, -0.19333205, -0.15259232, -0.20636759, -0.19216077,\n",
      "       -0.13799934, -0.1704073 , -0.11889694, -0.06846715, -0.16315524]), 'train_score': array([-0.00010768, -0.00012037, -0.0001037 , -0.0001115 , -0.00010574,\n",
      "       -0.00010675, -0.00011091, -0.00011846, -0.00011327, -0.00010955])}\n",
      "\n",
      "\n",
      "Neuroticism ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE: 0.272 MAE: 0.229\n",
      "Cross validation scores:  {'fit_time': array([0.08577037, 0.08876276, 0.09075761, 0.09075737, 0.08876228,\n",
      "       0.09175491, 0.09275246, 0.08876276, 0.08876252, 0.10272527]), 'score_time': array([0.0009973 , 0.0009973 , 0.        , 0.        , 0.00099754,\n",
      "       0.00099707, 0.00099754, 0.        , 0.        , 0.        ]), 'test_score': array([-0.24731634, -0.28952055, -0.28550231, -0.226978  , -0.26862398,\n",
      "       -0.17719642, -0.21708673, -0.16651636, -0.13700813, -0.21276448]), 'train_score': array([-1.01193481e-04, -1.14349597e-04, -1.09499962e-04, -1.04043176e-04,\n",
      "       -1.26215914e-04, -1.07254745e-04, -9.79392033e-05, -1.11966885e-04,\n",
      "       -1.15695043e-04, -1.17811752e-04])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Posts in tf-idf representation\n",
    "\n",
    "# Let's train type indicator individually\n",
    "for l in range(len(type_indicators)):\n",
    "    print(\"\\n\\n{} ...\".format(type_indicators[l]))\n",
    "    \n",
    "    # Let's train type indicator individually\n",
    "    y_train_class = y_train[:,l]\n",
    "    y_test_class = y_test[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    model = svm.LinearSVR(C=10)\n",
    "\n",
    "    \n",
    "    clf = model.fit(X_train_seq_trunc, y_train_class)\n",
    "    \n",
    "    \n",
    "#     pca = PCA(n_components=2).fit(X_train_seq_trunc)\n",
    "    \n",
    "#     pca_2d = pca.transform(X_train_seq_trunc)\n",
    "    \n",
    "#     svmClassifier_2d =   svm.LinearSVC(C=10,\n",
    "#                           class_weight='balanced').fit(   pca_2d, y_train_class)\n",
    "    \n",
    "#     for i in range(0, pca_2d.shape[0]):\n",
    "#         if y_train_res[i] == 0:\n",
    "#             c1 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='r', s=50,marker='+')\n",
    "#         elif y_train_res[i] == 1:\n",
    "#             c2 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='g',    s=50,marker='o')\n",
    "    \n",
    "#     pl.legend([c1, c2], [type_indicators[l][0], type_indicators[l][1]])\n",
    "#     x_min, x_max = pca_2d[:, 0].min() - 1,   pca_2d[:,0].max() + 1\n",
    "#     y_min, y_max = pca_2d[:, 1].min() - 1,   pca_2d[:, 1].max() + 1\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),   np.arange(y_min, y_max, .01))\n",
    "#     Z = svmClassifier_2d.predict(np.c_[xx.ravel(),  yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     pl.contour(xx, yy, Z)\n",
    "#     pl.title('Support Vector Machine Decision Surface')\n",
    "#     pl.axis('off')\n",
    "#     pl.show()\n",
    "    \n",
    "    # make predictions  for test data\n",
    "    y_pred = model.predict(X_test_seq_trunc)\n",
    "    predictions = [value for value in y_pred]\n",
    "    def rmse(y, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred - y)**2))\n",
    "    rmse_scorer = sklearn.metrics.make_scorer(rmse, greater_is_better=False)\n",
    "    scores = cross_validate(clf, X_train_seq_trunc, y_train_class, cv=10, scoring=rmse_scorer)\n",
    "\n",
    "\n",
    "    # evaluate predictions\n",
    "#     accuracy = sklearn.metrics.accuracy_score(y_test, predictions)\n",
    "#     f1_score_measure = sklearn.metrics.f1_score(y_test, predictions)\n",
    "    rmse_val = rmse(y_test_class, predictions)\n",
    "    mae = sklearn.metrics.mean_absolute_error(y_test_class, predictions)\n",
    "    print(\" RMSE: {:.3f} MAE: {:.3f}\".format(rmse_val, mae))\n",
    "    print(\"Cross validation scores: \", scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "encoding = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 152\n"
     ]
    }
   ],
   "source": [
    "X, y = np.array(list_posts), np.array(list_personality)\n",
    "print (\"total examples %s\" % len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(GLOVE_DIR, glove_file), \"rb\") as lines:\n",
    "    wvec = {line.split()[0].decode(encoding): np.array(line.split()[1:],dtype=np.float32)\n",
    "               for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct \n",
    "\n",
    "glove_dict = {}\n",
    "all_words = set(w for words in X)\n",
    "with open(os.path.join(GLOVE_DIR, glove_file), \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_dict[word] = nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the classics - naive bayes of the multinomial and bernoulli varieties\n",
    "# with either pure counts or tfidf features\n",
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVR(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVR(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_dict))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_dict))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_dict)), \n",
    "                        (\"extra trees\", ExtraTreesRegressor(n_estimators=200))])\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_dict)), \n",
    "                        (\"extra trees\", ExtraTreesRegressor(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                score\n",
      "-----------------  -------\n",
      "svc                -0.1469\n",
      "glove_small        -0.1477\n",
      "glove_small_tfidf  -0.1477\n",
      "svc_tfidf          -0.1495\n",
      "Conscientiousness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                score\n",
      "-----------------  -------\n",
      "glove_small        -0.1507\n",
      "glove_small_tfidf  -0.1507\n",
      "svc_tfidf          -0.1514\n",
      "svc                -0.1571\n",
      "Exraversion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                score\n",
      "-----------------  -------\n",
      "glove_small        -0.1663\n",
      "glove_small_tfidf  -0.1663\n",
      "svc_tfidf          -0.1664\n",
      "svc                -0.1912\n",
      "Agreeableness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                score\n",
      "-----------------  -------\n",
      "glove_small        -0.1549\n",
      "glove_small_tfidf  -0.1549\n",
      "svc_tfidf          -0.1567\n",
      "svc                -0.1795\n",
      "Neuroticism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                score\n",
      "-----------------  -------\n",
      "svc_tfidf          -0.2212\n",
      "glove_small        -0.2237\n",
      "glove_small_tfidf  -0.2237\n",
      "svc                -0.2606\n"
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "#     (\"mult_nb\", mult_nb),\n",
    "#     (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "#     (\"bern_nb\", bern_nb),\n",
    "#     (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    (\"glove_small\", etree_glove_small),\n",
    "    (\"glove_small_tfidf\", etree_glove_small_tfidf),\n",
    "]\n",
    "\n",
    "\n",
    "type_indicators = [\"Openness\", \"Conscientiousness\", \"Exraversion\", \"Agreeableness\", \"Neuroticism\"]\n",
    "\n",
    "for l in range(len(type_indicators)):\n",
    "    print(type_indicators[l])\n",
    "    y = list_personality[:,l]\n",
    "    def rmse(y, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred - y)**2))\n",
    "    rmse_scorer = sklearn.metrics.make_scorer(rmse, greater_is_better=False)\n",
    "    scores = cross_validate(clf, X_train_seq_trunc, y_train_class, cv=10, scoring=rmse_scorer)\n",
    "\n",
    "    unsorted_scores = [(name, cross_validate(model, X, y, cv=5, scoring=rmse_scorer)['test_score'].mean()) for name, model in all_models]\n",
    "    scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "    print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
