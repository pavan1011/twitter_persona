{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT \n",
    "https://colab.research.google.com/drive/1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU#scrollTo=E_t4cM6KLc98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ProgramData\\\\Anaconda3\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# * File:    Twitter_Persona_GloVe.py\n",
    "# *\n",
    "# * Author1:  Pavan Kumar K N (pavankumar.karkekopp@ucalgary.ca)\n",
    "# * Date:     11th Aug 2019\n",
    "# * Summary of File:\n",
    "# * Explore mbti_1.csv file acquired from https://www.kaggle.com/datasnaek/mbti-type\n",
    "# * Apply state-of-the-art reported publicly\n",
    "# * Build classifier model that is better using machine learning techniques\n",
    "\n",
    "#Just making sure the right environment is running this script\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "# # Load pre-trained model tokenizer (vocabulary)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Read Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import pylab as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_types(row):\n",
    "    t=row['type']\n",
    "\n",
    "    I = 0\n",
    "    N = 0\n",
    "    T = 0\n",
    "    J = 0\n",
    "    \n",
    "    if t[0] == 'I': I = 1\n",
    "    elif t[0] == 'E': I = 0\n",
    "    else: print('Could not identify label for I-E')\n",
    "        \n",
    "    if t[1] == 'N': N = 1\n",
    "    elif t[1] == 'S': N = 0\n",
    "    else: print('Could not identify label for N-S')\n",
    "        \n",
    "    if t[2] == 'T': T = 1\n",
    "    elif t[2] == 'F': T = 0\n",
    "    else: print('Could not identify label for T-F')\n",
    "        \n",
    "    if t[3] == 'J': J = 1\n",
    "    elif t[3] == 'P': J = 0\n",
    "    else: print('Could not identify label for J-P')\n",
    "    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to binarize the types into simple lists instead of pandas.series\n",
    "personality_binary = {'I':1, 'E':0, 'N':1,'S':0, 'T':1, 'F':0, 'J':1, 'P': 0}\n",
    "binary_personality = [{1:'I', 0:'E'}, \n",
    "                      {1:'N', 0:'S'},\n",
    "                      {1:'T', 0:'F'},\n",
    "                      {1:'J', 0:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    return [personality_binary[l] for l in personality]\n",
    "\n",
    "\n",
    "def translate_binary(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += binary_personality[i][l]\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMBTI(mbti_file_path): \n",
    "    \n",
    "    \n",
    "    #List of strings to remove from the corpus\n",
    "    unique_type_list = ['INFJ', \n",
    "                        'ENTP', \n",
    "                        'INTP', \n",
    "                        'INTJ', \n",
    "                        'ENTJ', \n",
    "                        'ENFJ', \n",
    "                        'INFP', \n",
    "                        'ENFP',\n",
    "                        'ISFP', \n",
    "                        'ISTP', \n",
    "                        'ISFJ', \n",
    "                        'ISTJ', \n",
    "                        'ESTP', \n",
    "                        'ESFP', \n",
    "                        'ESTJ', \n",
    "                        'ESFJ']\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    \n",
    "\n",
    "    \n",
    "    # Initialize for Lemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "    #List of unique types of personality\n",
    "    unique_type_list = [x.lower() for x in unique_type_list]\n",
    "\n",
    "    #Read file\n",
    "    mbti_data = pd.read_csv(mbti_file_path)\n",
    "\n",
    "\n",
    "    raw_posts = mbti_data.posts.values\n",
    "    filtered_posts = [p.split(\"|||\") for p in raw_posts]\n",
    "    mbti_data_encoded = mbti_data.join(mbti_data.apply(lambda row: encode_types(row), axis=1))\n",
    "    \n",
    "    len_data = len(mbti_data_encoded)\n",
    "    i=0\n",
    "    \n",
    "    \n",
    "    for row in mbti_data_encoded.iterrows():\n",
    "        i+=1\n",
    "        tweets = []\n",
    "\n",
    "        if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "            print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments\n",
    "        posts = row[1].posts\n",
    "        \n",
    "        for tweet_string in posts.split(\"|||\"):\n",
    "            #Removing mentions\n",
    "            tweet_string = tweet_string.replace('@username', '')\n",
    "            #Removing unecessary spaces\n",
    "\n",
    "            #Removing URL\n",
    "            tweet_string = re.sub(\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", tweet_string)\n",
    "            tweet_string = tweet_string.strip()\n",
    "            tweets.append(tweet_string)\n",
    "\n",
    "        j=0\n",
    "        for pos in tweets:\n",
    "            if pos is not None:\n",
    "                pos = re.sub(\"[^a-zA-Z]\", \" \", pos)\n",
    "                pos = re.sub(\" +\", \" \", pos).lower()\n",
    "                pos = \" \".join([lemmatiser.lemmatize(w) for w in pos.split(' ')])\n",
    "\n",
    "                if pos!= \" \":\n",
    "                    tweets[j] = pos\n",
    "                else:\n",
    "                    tweets[j] = None\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        tweets = list(filter(None, tweets))\n",
    "\n",
    "        #'Add [SEP] tokens for BERT tokenizer'\n",
    "        processed_tweets = '[SEP]'.join(tweets)\n",
    "        list_posts.append(processed_tweets)\n",
    "        list_personality.append(translate_personality(row[1].type))\n",
    "    return np.array(list_posts), np.array(list_personality)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "#         temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "#         temp = re.sub(' +', ' ', temp).strip().lower()\n",
    "#         if remove_stop_words:\n",
    "#             temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in stopwords.words(\"english\")])\n",
    "#         else:\n",
    "#             temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "            \n",
    "#         if remove_mbti_profiles:\n",
    "#             for t in unique_type_list:\n",
    "#                 temp = temp.replace(t,\"\")\n",
    "\n",
    "#         type_labelized = translate_personality(row[1].type)\n",
    "#         list_personality.append(type_labelized)\n",
    "#         list_posts.append(temp)\n",
    "\n",
    "#     list_posts = np.array(list_posts)\n",
    "#     list_personality = np.array(list_personality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3887844cb2bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist_posts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_personality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparseMBTI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/mbti_1.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-962e513de498>\u001b[0m in \u001b[0;36mparseMBTI\u001b[1;34m(mbti_file_path)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^a-zA-Z]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" +\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m!=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-962e513de498>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^a-zA-Z]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" +\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlemmatiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m!=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_posts, list_personality = parseMBTI(\"data/mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_posts[0], list_personality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "bert_model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify max_token_length according to above\n",
    "max_token_length = 256\n",
    "list_posts_vec = []\n",
    "list_personality_vec = []\n",
    "list_posts_len = len(list_posts)\n",
    "\n",
    "count_progress = 0\n",
    "for posts, personality in zip(list_posts, list_personality):\n",
    "    \n",
    "    count_progress += 1\n",
    "    sentence_list = posts.split('[SEP]')\n",
    "    running_token_len = 0\n",
    "    sentence_count = 0\n",
    "    tweet_stream_vec_list = []\n",
    "    text_batch_list = []\n",
    "    text = \"\"\n",
    "    \n",
    "    if (count_progress % 100 == 0 or count_progress == 1 or count_progress == list_posts_len):\n",
    "            print(\"{} of {} rows\".format(count_progress, list_posts_len))\n",
    "            \n",
    "    #Split the sequence of tweets into separate batches of max_token_length\n",
    "    for sentence in sentence_list:\n",
    "        \n",
    "        sentence_len = len(sentence.strip().split(' '))\n",
    "#         print(\" \\Sentence Length: {}\\n  \\nRunning Token Length: {}\".format(sentence_len, running_token_len))\n",
    "        #Case 1: Sentence is smaller than max token_length\n",
    "        if(sentence_len <= max_token_length):\n",
    "           \n",
    "           #Concatenate into single post\n",
    "            if(running_token_len + sentence_len < max_token_length):\n",
    "                running_token_len += sentence_len\n",
    "                text += sentence\n",
    "            \n",
    "            else:\n",
    "                if text!= \"\":\n",
    "                    text_batch_list.append(text)\n",
    "                sentence_count += 1\n",
    "                running_token_len = sentence_len\n",
    "                text = sentence\n",
    "           \n",
    "        \n",
    "        else:\n",
    "            if text!= \"\":\n",
    "                text_batch_list.append(text)\n",
    "            text_len = len(sentence.strip().split(' '))\n",
    "            \n",
    "            while(text_len > max_token_length):\n",
    "                text_batch_list.append( \" \".join(sentence.strip().split(' ')[:max_token_length]))\n",
    "                sentence = \" \".join(sentence.strip().split(' ')[max_token_length:])\n",
    "                text_len = len(sentence.strip().split(' '))\n",
    "            \n",
    "            running_token_len = sentence_len\n",
    "            text = sentence\n",
    "        \n",
    "#     print(\"Total batches: {}\\n{}\".format(len(text_batch_list), [len(text.strip().split(' ')) for text in text_batch_list]))\n",
    "\n",
    "    #Process the batches\n",
    "    for text in text_batch_list:\n",
    "        marked_text = \"[CLS]\" + text + \"[SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#         print(marked_text)\n",
    "#         print((tokenized_text))\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "        \n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        \n",
    "#         print(tokens_tensor.shape, segments_tensors.shape)\n",
    "        # Predict hidden states features for each layer\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = bert_model(tokens_tensor, segments_tensors)\n",
    "            \n",
    "        token_embeddings = [] \n",
    "\n",
    "        batch_i = 0 #Since we have only one sentence\n",
    "        # For each token in the sentence...\n",
    "        for token_i in range(len(tokenized_text)):\n",
    "\n",
    "            # Holds 12 layers of hidden states for each token \n",
    "            hidden_layers = [] \n",
    "\n",
    "            # For each of the 12 layers...\n",
    "            for layer_i in range(len(encoded_layers)):\n",
    "\n",
    "                # Lookup the vector for `token_i` in `layer_i`\n",
    "                vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "                hidden_layers.append(vec)\n",
    "\n",
    "        token_embeddings.append(hidden_layers)\n",
    "\n",
    "        # Stores the token vectors, with shape [22 x 768]\n",
    "        token_vecs_sum = []\n",
    "\n",
    "        # For each token in the sentence...\n",
    "        for token in token_embeddings:\n",
    "            # Sum the vectors from the last four layers.\n",
    "            sum_vec = torch.sum(torch.stack(token)[-4:], 0)\n",
    "\n",
    "            # Use `sum_vec` to represent `token`.\n",
    "            token_vecs_sum.append(sum_vec)\n",
    "        \n",
    "        sentence_embedding = torch.mean(encoded_layers[11], 1)\n",
    "        tweet_stream_vec_list.append(sentence_embedding)\n",
    "        \n",
    "    #Concatenate the stream vector into one vector to represent the whole stream\n",
    "    if(len(tweet_stream_vec_list) > 0):\n",
    "        tweet_stream_vec = torch.mean(torch.stack(tweet_stream_vec_list), dim=0)\n",
    "        list_posts_vec.append(tweet_stream_vec.numpy().ravel())\n",
    "        list_personality_vec.append(personality)\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list_posts_vec).shape, np.array(list_personality_vec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(list_posts_vec), np.array(list_personality_vec), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred - y)**2))\n",
    "\n",
    "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "def eval_metric(history, metric_name):\n",
    "    '''\n",
    "    Function to evaluate a trained model on a chosen metric. \n",
    "    Training and validation metric are plotted in a\n",
    "    line chart for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        history : model training history\n",
    "        metric_name : loss or accuracy\n",
    "    Output:\n",
    "        line chart with epochs of x-axis and metric on\n",
    "        y-axis\n",
    "    '''\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
    "    '''\n",
    "    Function to test the model on new data after training it\n",
    "    on the full training data with the optimal number of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        model : trained model\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_test : test features\n",
    "        y_test : test target\n",
    "        epochs : optimal number of epochs\n",
    "    Output:\n",
    "        test accuracy and test loss\n",
    "    '''\n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_indicators = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) – Sensing (S)\", \n",
    "                   \"FT: Feeling (F) - Thinking (T)\", \"JP: Judging (J) – Perceiving (P)\"  ]\n",
    "\n",
    "for l in range(len(type_indicators)):\n",
    "    print(type_indicators[l])\n",
    "    print(y_test[:,l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(type_indicators)):\n",
    "    print(\"\\n\\n{} ...\".format(type_indicators[l]))\n",
    "    \n",
    "    y_train_class = y_train[:,l]\n",
    "    y_test_class = y_test[:,l]\n",
    "    \n",
    "    print(X_train.shape, y_train_class.shape)\n",
    "    seed = 7    \n",
    "    model = XGBClassifier(learning_rate=0.01,\n",
    "                             n_estimators=100,\n",
    "                             max_depth=6,\n",
    "                             min_child_weight=6,\n",
    "                             colsample_bytree=0.7,\n",
    "                             objective='reg:logistic',\n",
    "                             nthread=8,\n",
    "                             scale_pos_weight=1,\n",
    "                             seed=7)\n",
    "\n",
    "    model.fit(X_train, y_train_class)\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    prediction_proba = [value for value in y_pred]\n",
    "#     # evaluate predictions\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_test_class, predictions)\n",
    "    f1_score_measure = sklearn.metrics.f1_score(y_test_class, predictions)\n",
    "    precision_measure = sklearn.metrics.precision_score(y_test_class, predictions)\n",
    "    recall_measure = sklearn.metrics.recall_score(y_test_class, predictions)\n",
    "    auc_roc = sklearn.metrics.roc_auc_score(y_test_class, predictions)\n",
    "#     mae = sklearn.metrics.mean_absolute_error(y_test_class, prediction_proba)\n",
    "    print(\" Accuracy: {:.3f} Precision: {:.3f} Recall {:.3f} F1-score {:.3f} ROC-AUC {:.3f}\".format(accuracy, \n",
    "                                                                                                    precision_measure, \n",
    "                                                                                                    recall_measure,\n",
    "                                                                                                   auc_roc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor, AdaBoostRegressor, AdaBoostClassifier\n",
    "def rmse(y, y_pred):\n",
    "        return np.sqrt(np.mean((y_pred - y)**2))\n",
    "rmse_scorer = sklearn.metrics.make_scorer(rmse, greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "IE: Introversion (I) / Extroversion (E) ...\n",
      "(6907, 768) (6907,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE test: 0.4788521306805733 RMSE cv5: {'fit_time': array([2.52526259, 2.5362339 , 2.58612847, 2.56715107, 2.50232363]), 'score_time': array([0.194628  , 0.18550563, 0.18859291, 0.18550563, 0.18550563]), 'test_acc': array([0.77141204, 0.76909722, 0.76882966, 0.76825029, 0.77056779]), 'train_acc': array([1., 1., 1., 1., 1.]), 'test_prec': array([0.73591385, 0.63501742, 0.61635008, 0.6080536 , 0.67760599]), 'train_prec': array([1., 1., 1., 1., 1.]), 'test_rec': array([0.50764326, 0.50263073, 0.50490215, 0.50628519, 0.50691144]), 'train_rec': array([1., 1., 1., 1., 1.]), 'test_f1': array([0.87036429, 0.86922321, 0.86879316, 0.86824769, 0.86982249]), 'train_f1': array([1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.50764326, 0.50263073, 0.50490215, 0.50628519, 0.50691144]), 'train_roc_auc': array([1., 1., 1., 1., 1.])} RMSE cv10: {'fit_time': array([2.89527583, 2.91923833, 2.95810771, 2.92124009, 2.94414306,\n",
      "       2.9192121 , 2.96907949, 2.89029241, 2.9714458 , 2.93331194]), 'score_time': array([0.10671544, 0.10871005, 0.11469746, 0.10970354, 0.108711  ,\n",
      "       0.10870934, 0.10973334, 0.10671592, 0.1104188 , 0.10871077]), 'test_acc': array([0.7699422 , 0.77225434, 0.76940904, 0.76825029, 0.77056779,\n",
      "       0.77288528, 0.76709154, 0.76709154, 0.77172654, 0.76825029]), 'train_acc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_prec': array([0.68546512, 0.78604651, 0.38470452, 0.38457077, 0.71879845,\n",
      "       0.88604651, 0.57288012, 0.57288012, 0.71956437, 0.59979973]), 'train_prec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_rec': array([0.50599624, 0.50924812, 0.5       , 0.49924699, 0.50427211,\n",
      "       0.50753769, 0.50377263, 0.50377263, 0.50854423, 0.50452564]), 'train_rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_f1': array([0.8695082 , 0.87081967, 0.86967911, 0.8689384 , 0.87007874,\n",
      "       0.87139108, 0.8676761 , 0.8676761 , 0.87047995, 0.86842105]), 'train_f1': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.50599624, 0.50924812, 0.5       , 0.49924699, 0.50427211,\n",
      "       0.50753769, 0.50377263, 0.50377263, 0.50854423, 0.50452564]), 'train_roc_auc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "\n",
      "\n",
      "NS: Intuition (N) – Sensing (S) ...\n",
      "(6907, 768) (6907,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE test: 0.37818326577603667 RMSE cv5: {'fit_time': array([2.69784021, 2.65193081, 2.67683196, 2.72273397, 2.67887926]), 'score_time': array([0.180516  , 0.17752552, 0.18151641, 0.18450737, 0.17952895]), 'test_acc': array([0.86111111, 0.86284722, 0.86210892, 0.86326767, 0.86210892]), 'train_acc': array([1., 1., 1., 1., 1.]), 'test_prec': array([0.43105446, 0.9313839 , 0.43130435, 0.9315942 , 0.43130435]), 'train_prec': array([1., 1., 1., 1., 1.]), 'test_rec': array([0.49932886, 0.50210084, 0.4996642 , 0.5021097 , 0.4996642 ]), 'train_rec': array([1., 1., 1., 1., 1.]), 'test_f1': array([0.92537313, 0.92632888, 0.92594897, 0.92657125, 0.92594897]), 'train_f1': array([1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.49932886, 0.50210084, 0.4996642 , 0.5021097 , 0.4996642 ]), 'train_roc_auc': array([1., 1., 1., 1., 1.])} RMSE cv10: {'fit_time': array([3.29728341, 3.1528511 , 3.18051481, 3.28326988, 3.17948985,\n",
      "       3.24984598, 3.27916217, 3.24469447, 3.23298216, 3.31627941]), 'score_time': array([0.1093905 , 0.11070609, 0.1127274 , 0.10970545, 0.10873747,\n",
      "       0.11575794, 0.10870934, 0.10871053, 0.10970783, 0.11266994]), 'test_acc': array([0.86111111, 0.86226852, 0.86226852, 0.86342593, 0.86226852,\n",
      "       0.86226852, 0.86342593, 0.86194896, 0.86078886, 0.86310905]), 'train_acc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_prec': array([0.43105446, 0.43113426, 0.43113426, 0.93163384, 0.68155452,\n",
      "       0.43113426, 0.93163384, 0.43147503, 0.43139535, 0.43155452]), 'train_prec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_rec': array([0.49932886, 0.5       , 0.5       , 0.50420168, 0.50353054,\n",
      "       0.5       , 0.50420168, 0.49932796, 0.49865591, 0.5       ]), 'train_rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_f1': array([0.92537313, 0.92604102, 0.92604102, 0.92661692, 0.92594897,\n",
      "       0.92604102, 0.92661692, 0.9258567 , 0.92518703, 0.92652553]), 'train_f1': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.49932886, 0.5       , 0.5       , 0.50420168, 0.50353054,\n",
      "       0.5       , 0.50420168, 0.49932796, 0.49865591, 0.5       ]), 'train_roc_auc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "\n",
      "\n",
      "FT: Feeling (F) - Thinking (T) ...\n",
      "(6907, 768) (6907,)\n",
      " RMSE test: 0.5155373964903818 RMSE cv5: {'fit_time': array([2.59115028, 2.60825491, 2.6429491 , 2.63895917, 2.52967095]), 'score_time': array([0.1934855 , 0.20146275, 0.19049191, 0.20146322, 0.19347572]), 'test_acc': array([0.72727273, 0.73943254, 0.73190504, 0.74696005, 0.70857474]), 'train_acc': array([1., 1., 1., 1., 1.]), 'test_prec': array([0.72929913, 0.74032568, 0.73529721, 0.74736356, 0.70742197]), 'train_prec': array([1., 1., 1., 1., 1.]), 'test_rec': array([0.71964498, 0.73309566, 0.72363339, 0.74130273, 0.70318144]), 'train_rec': array([1., 1., 1., 1., 1.]), 'test_f1': array([0.67849829, 0.69798658, 0.68090972, 0.70924817, 0.66754792]), 'train_f1': array([1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.71964498, 0.73309566, 0.72363339, 0.74130273, 0.70318144]), 'train_roc_auc': array([1., 1., 1., 1., 1.])} RMSE cv10: {'fit_time': array([2.92961907, 2.95729947, 3.05436873, 2.95144486, 2.99748611,\n",
      "       3.06031489, 3.09277892, 2.90235758, 2.94223881, 2.90335178]), 'score_time': array([0.10870957, 0.11170244, 0.1117003 , 0.11083364, 0.11077285,\n",
      "       0.11469436, 0.10970783, 0.10769987, 0.10870934, 0.10771775]), 'test_acc': array([0.73032407, 0.74305556, 0.7349537 , 0.75578704, 0.73232908,\n",
      "       0.74044032, 0.73232908, 0.74391657, 0.73580533, 0.68829664]), 'train_acc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_prec': array([0.73021113, 0.7485618 , 0.73373045, 0.75755996, 0.73622142,\n",
      "       0.74358183, 0.731966  , 0.74352342, 0.73837761, 0.68607493]), 'train_prec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_rec': array([0.7244561 , 0.73407148, 0.73067211, 0.74932012, 0.72388229,\n",
      "       0.73272068, 0.72695369, 0.73900407, 0.72824606, 0.68569258]), 'train_rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_f1': array([0.689747  , 0.6908078 , 0.70143416, 0.71601615, 0.68049793,\n",
      "       0.69315068, 0.69403974, 0.7088274 , 0.68852459, 0.65819568]), 'train_f1': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.7244561 , 0.73407148, 0.73067211, 0.74932012, 0.72388229,\n",
      "       0.73272068, 0.72695369, 0.73900407, 0.72824606, 0.68569258]), 'train_roc_auc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "\n",
      "\n",
      "JP: Judging (J) – Perceiving (P) ...\n",
      "(6907, 768) (6907,)\n",
      " RMSE test: 0.5884430301808192 RMSE cv5: {'fit_time': array([2.82669258, 2.87333441, 2.83553433, 2.79854536, 2.89524555]), 'score_time': array([0.21742034, 0.19946814, 0.20246077, 0.21740651, 0.20744658]), 'test_acc': array([0.625     , 0.63252315, 0.64484357, 0.63904983, 0.62514484]), 'train_acc': array([1., 1., 1., 1., 1.]), 'test_prec': array([0.60338699, 0.61384178, 0.65424362, 0.62647065, 0.60088456]), 'train_prec': array([1., 1., 1., 1., 1.]), 'test_rec': array([0.55278282, 0.56556261, 0.56893197, 0.57146577, 0.55667568]), 'train_rec': array([1., 1., 1., 1., 1.]), 'test_f1': array([0.30322581, 0.34468524, 0.31354983, 0.35171696, 0.32533889]), 'train_f1': array([1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.55278282, 0.56556261, 0.56893197, 0.57146577, 0.55667568]), 'train_roc_auc': array([1., 1., 1., 1., 1.])} RMSE cv10: {'fit_time': array([3.33809423, 3.31016898, 3.36402512, 3.31832361, 3.20345879,\n",
      "       3.22772884, 3.22038078, 3.1645577 , 3.24035525, 3.21841407]), 'score_time': array([0.12267303, 0.12067819, 0.11968112, 0.11668563, 0.11568761,\n",
      "       0.11665869, 0.11868358, 0.11569166, 0.11668873, 0.11968136]), 'test_acc': array([0.63541667, 0.63888889, 0.63657407, 0.6400463 , 0.64814815,\n",
      "       0.64351852, 0.63078704, 0.63921114, 0.61948956, 0.62064965]), 'train_acc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_prec': array([0.62696942, 0.6239997 , 0.62234998, 0.62681406, 0.64826766,\n",
      "       0.63995354, 0.61708372, 0.63011964, 0.5917137 , 0.59812951]), 'train_prec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_rec': array([0.56215971, 0.57360355, 0.56866304, 0.57405727, 0.57824158,\n",
      "       0.57289776, 0.55732002, 0.56881083, 0.54692364, 0.5428175 ]), 'train_rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_f1': array([0.31372549, 0.36326531, 0.34583333, 0.3613963 , 0.35319149,\n",
      "       0.34188034, 0.30501089, 0.33688699, 0.29310345, 0.26185102]), 'train_f1': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.56215971, 0.57360355, 0.56866304, 0.57405727, 0.57824158,\n",
      "       0.57289776, 0.55732002, 0.56881083, 0.54692364, 0.5428175 ]), 'train_roc_auc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(type_indicators)):\n",
    "    print(\"\\n\\n{} ...\".format(type_indicators[l]))\n",
    "    \n",
    "    y_train_class = y_train[:,l]\n",
    "    y_test_class = y_test[:,l]\n",
    "    \n",
    "    print(X_train.shape, y_train_class.shape)\n",
    "    seed = 7    \n",
    "    \n",
    "    model = ExtraTreesClassifier(n_estimators=100)\n",
    "\n",
    "    model.fit(X_train, y_train_class)\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "#     # evaluate predictions\n",
    "#     accuracy = sklearn.metrics.accuracy_score(y_test_class, predictions)\n",
    "#     f1_score_measure = sklearn.metrics.f1_score(y_test_class, predictions)\n",
    "    rmse_val = rmse(y_test_class, predictions)\n",
    "    scoring = {'acc': 'accuracy',\n",
    "               'prec': 'precision_macro',\n",
    "               'rec': 'recall_macro',\n",
    "               'f1': sklearn.metrics.make_scorer(sklearn.metrics.f1_score),\n",
    "               'roc_auc': sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score)}\n",
    "    rmse_cv_5 = cross_validate(model, np.array(list_posts_vec), np.array(list_personality_vec)[:,l], cv=5, scoring=scoring)\n",
    "    rmse_cv_10 = cross_validate(model,np.array(list_posts_vec), np.array(list_personality_vec)[:,l], cv=10, scoring=scoring)\n",
    "    print(\" RMSE test: {} RMSE cv5: {} RMSE cv10: {}\".format(rmse_val, rmse_cv_5, rmse_cv_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "IE: Introversion (I) / Extroversion (E) ...\n",
      "(6907, 768) (6907,)\n",
      " RMSE test: 0.5884430301808192 RMSE cv5: {'fit_time': array([1.38729429, 1.41123462, 1.39530492, 1.44916201, 1.38034463]), 'score_time': array([0.09973335, 0.10175776, 0.10073161, 0.10172868, 0.09873652]), 'test_acc': array([0.76851852, 0.76851852, 0.76535342, 0.76651217, 0.76651217]), 'train_acc': array([1., 1., 1., 1., 1.]), 'test_prec': array([0.62274029, 0.62534744, 0.57968408, 0.58981318, 0.58144196]), 'train_prec': array([1., 1., 1., 1., 1.]), 'test_rec': array([0.50751597, 0.50926979, 0.50792176, 0.507795  , 0.50603545]), 'train_rec': array([1., 1., 1., 1., 1.]), 'test_f1': array([0.86833443, 0.86816084, 0.86602713, 0.86686488, 0.86704058]), 'train_f1': array([1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.50751597, 0.50926979, 0.50792176, 0.507795  , 0.50603545]), 'train_roc_auc': array([1., 1., 1., 1., 1.])} RMSE cv10: {'fit_time': array([1.63164616, 1.6047461 , 1.60674191, 1.59577036, 1.61272526,\n",
      "       1.62865257, 1.6177125 , 1.61668634, 1.5987339 , 1.61972499]), 'score_time': array([0.05884528, 0.05784822, 0.05784464, 0.05684757, 0.05884242,\n",
      "       0.05881572, 0.05785346, 0.05884314, 0.05884314, 0.05784369]), 'test_acc': array([0.7699422 , 0.7699422 , 0.76825029, 0.7636153 , 0.76477404,\n",
      "       0.77867903, 0.77172654, 0.7589803 , 0.76477404, 0.76825029]), 'train_acc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_prec': array([0.66387591, 0.65914413, 0.6080536 , 0.38403263, 0.58083498,\n",
      "       0.78213865, 0.69904971, 0.51665004, 0.50979532, 0.61342296]), 'train_prec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_rec': array([0.50949248, 0.5112406 , 0.50628519, 0.49623494, 0.50930481,\n",
      "       0.52537915, 0.51030378, 0.50202065, 0.50050705, 0.50804474]), 'train_rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_f1': array([0.86916502, 0.86899276, 0.86824769, 0.86596583, 0.86547382,\n",
      "       0.87376074, 0.87030941, 0.86206897, 0.86635945, 0.86807388]), 'train_f1': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.50949248, 0.5112406 , 0.50628519, 0.49623494, 0.50930481,\n",
      "       0.52537915, 0.51030378, 0.50202065, 0.50050705, 0.50804474]), 'train_roc_auc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "\n",
      "\n",
      "NS: Intuition (N) – Sensing (S) ...\n",
      "(6907, 768) (6907,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE test: 0.5884430301808192 RMSE cv5: {'fit_time': array([1.48404479, 1.47309756, 1.48706031, 1.48406744, 1.46212959]), 'score_time': array([0.10471725, 0.10073113, 0.09474969, 0.10073423, 0.09886718]), 'test_acc': array([0.86111111, 0.86284722, 0.86384705, 0.86326767, 0.86095017]), 'train_acc': array([1., 1., 1., 1., 1.]), 'test_prec': array([0.5562645 , 0.73180499, 0.93184455, 0.76513832, 0.43122461]), 'train_prec': array([1., 1., 1., 1., 1.]), 'test_rec': array([0.50109413, 0.50563138, 0.50421941, 0.50388361, 0.49899261]), 'train_rec': array([1., 1., 1., 1., 1.]), 'test_f1': array([0.9253267 , 0.92623716, 0.92685963, 0.92652553, 0.9252802 ]), 'train_f1': array([1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.50109413, 0.50563138, 0.50421941, 0.50388361, 0.49899261]), 'train_roc_auc': array([1., 1., 1., 1., 1.])} RMSE cv10: {'fit_time': array([1.71043587, 1.72442722, 1.70148993, 1.68852282, 1.75634456,\n",
      "       1.72149014, 1.71143341, 1.70352864, 1.72240806, 1.70444608]), 'score_time': array([0.05981278, 0.05784488, 0.05784345, 0.056849  , 0.06180406,\n",
      "       0.05981255, 0.05884314, 0.05784535, 0.05783963, 0.05684614]), 'test_acc': array([0.86226852, 0.8587963 , 0.86574074, 0.86226852, 0.86226852,\n",
      "       0.86226852, 0.86226852, 0.86426914, 0.86194896, 0.86078886]), 'train_acc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_prec': array([0.43113426, 0.43089431, 0.93263647, 0.68155452, 0.43113426,\n",
      "       0.43113426, 0.68155452, 0.93205575, 0.43147503, 0.43139535]), 'train_prec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_rec': array([0.5       , 0.49798658, 0.51260504, 0.50353054, 0.5       ,\n",
      "       0.5       , 0.50353054, 0.50423729, 0.49932796, 0.49865591]), 'train_rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_f1': array([0.92604102, 0.92403487, 0.92777086, 0.92594897, 0.92604102,\n",
      "       0.92604102, 0.92594897, 0.9271028 , 0.9258567 , 0.92518703]), 'train_f1': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.5       , 0.49798658, 0.51260504, 0.50353054, 0.5       ,\n",
      "       0.5       , 0.50353054, 0.50423729, 0.49932796, 0.49865591]), 'train_roc_auc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "\n",
      "\n",
      "FT: Feeling (F) - Thinking (T) ...\n",
      "(6907, 768) (6907,)\n",
      " RMSE test: 0.5884430301808192 RMSE cv5: {'fit_time': array([1.34840083, 1.39926386, 1.34640384, 1.35637689, 1.36034322]), 'score_time': array([0.09873652, 0.09774137, 0.09674001, 0.0977416 , 0.09973431]), 'test_acc': array([0.71627099, 0.73190504, 0.72785177, 0.72669369, 0.71320973]), 'train_acc': array([1., 1., 1., 1., 1.]), 'test_prec': array([0.71741032, 0.73182387, 0.73292486, 0.72744269, 0.71225552]), 'train_prec': array([1., 1., 1., 1., 1.]), 'test_rec': array([0.70880867, 0.72614379, 0.71853832, 0.7199792 , 0.70775204]), 'train_rec': array([1., 1., 1., 1., 1.]), 'test_f1': array([0.66666667, 0.69194943, 0.67132867, 0.6819407 , 0.67240238]), 'train_f1': array([1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.70880867, 0.72614379, 0.71853832, 0.7199792 , 0.70775204]), 'train_roc_auc': array([1., 1., 1., 1., 1.])} RMSE cv10: {'fit_time': array([1.55983758, 1.55983734, 1.55385327, 1.56781602, 1.61172724,\n",
      "       1.58078146, 1.57579255, 1.58976483, 1.58576989, 1.6107285 ]), 'score_time': array([0.05784559, 0.05784607, 0.05784583, 0.05784607, 0.05984116,\n",
      "       0.05983877, 0.05984092, 0.05883265, 0.05884194, 0.05983925]), 'test_acc': array([0.72453704, 0.73032407, 0.7349537 , 0.74421296, 0.71378911,\n",
      "       0.71958285, 0.7161066 , 0.74507532, 0.72421784, 0.68829664]), 'train_acc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_prec': array([0.72338534, 0.73726764, 0.73362072, 0.74732669, 0.71652631,\n",
      "       0.72591145, 0.71999977, 0.74633969, 0.73007669, 0.68606936]), 'train_prec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_rec': array([0.71969697, 0.7201826 , 0.73086636, 0.73649961, 0.70521597,\n",
      "       0.70941752, 0.70697337, 0.73873099, 0.71446802, 0.68434884]), 'train_rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_f1': array([0.68766404, 0.67043847, 0.70221066, 0.69767442, 0.65836791,\n",
      "       0.6572238 , 0.65829847, 0.70430108, 0.66478873, 0.65200517]), 'train_f1': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.71969697, 0.7201826 , 0.73086636, 0.73649961, 0.70521597,\n",
      "       0.70941752, 0.70697337, 0.73873099, 0.71446802, 0.68434884]), 'train_roc_auc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "\n",
      "\n",
      "JP: Judging (J) – Perceiving (P) ...\n",
      "(6907, 768) (6907,)\n",
      " RMSE test: 0.5884430301808192 RMSE cv5: {'fit_time': array([1.51392984, 1.47403646, 1.52592826, 1.48603487, 1.46608806]), 'score_time': array([0.10275507, 0.10272574, 0.10272598, 0.10272598, 0.10372305]), 'test_acc': array([0.6255787 , 0.6255787 , 0.63615295, 0.62977984, 0.62804171]), 'train_acc': array([1., 1., 1., 1., 1.]), 'test_prec': array([0.60150834, 0.60225226, 0.62260848, 0.61053147, 0.60466574]), 'train_prec': array([1., 1., 1., 1., 1.]), 'test_rec': array([0.55779895, 0.55653862, 0.56704742, 0.5602581 , 0.56235743]), 'train_rec': array([1., 1., 1., 1., 1.]), 'test_f1': array([0.32953368, 0.32251309, 0.33894737, 0.32665964, 0.34489796]), 'train_f1': array([1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.55779895, 0.55653862, 0.56704742, 0.5602581 , 0.56235743]), 'train_roc_auc': array([1., 1., 1., 1., 1.])} RMSE cv10: {'fit_time': array([1.69849539, 1.7263906 , 1.70345402, 1.71043301, 1.70142913,\n",
      "       1.77725363, 1.72339606, 1.69448495, 1.7034564 , 1.70542026]), 'score_time': array([0.06083703, 0.06183505, 0.06180477, 0.06183457, 0.06183529,\n",
      "       0.06083775, 0.06083989, 0.05980659, 0.06083298, 0.06283259]), 'test_acc': array([0.62847222, 0.62731481, 0.62268519, 0.625     , 0.63657407,\n",
      "       0.63310185, 0.62037037, 0.64269142, 0.63109049, 0.63805104]), 'train_acc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_prec': array([0.60744084, 0.60388446, 0.59529412, 0.5982906 , 0.62406855,\n",
      "       0.62131295, 0.59235617, 0.63005356, 0.61156054, 0.62557441]), 'train_prec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_rec': array([0.55994152, 0.5610002 , 0.55716878, 0.56261343, 0.56715064,\n",
      "       0.560244  , 0.55122   , 0.5777689 , 0.56310614, 0.56937088]), 'train_rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_f1': array([0.32985386, 0.34016393, 0.33739837, 0.35714286, 0.33755274,\n",
      "       0.31236443, 0.31380753, 0.37142857, 0.3375    , 0.34453782]), 'train_f1': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'test_roc_auc': array([0.55994152, 0.5610002 , 0.55716878, 0.56261343, 0.56715064,\n",
      "       0.560244  , 0.55122   , 0.5777689 , 0.56310614, 0.56937088]), 'train_roc_auc': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n"
     ]
    }
   ],
   "source": [
    "for l in range(len(type_indicators)):\n",
    "    print(\"\\n\\n{} ...\".format(type_indicators[l]))\n",
    "    \n",
    "    y_train_class = y_train[:,l]\n",
    "    y_test_class = y_test[:,l]\n",
    "    \n",
    "    print(X_train.shape, y_train_class.shape)\n",
    "    seed = 7    \n",
    "    \n",
    "    model = AdaBoostClassifier(ExtraTreesClassifier(n_estimators=50), \n",
    "                              n_estimators=50, \n",
    "                              learning_rate =0.1, \n",
    "                              random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train_class)\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "#     # evaluate predictions\n",
    "#     accuracy = sklearn.metrics.accuracy_score(y_test_class, predictions)\n",
    "#     f1_score_measure = sklearn.metrics.f1_score(y_test_class, predictions)\n",
    "    scoring = {'acc': 'accuracy',\n",
    "               'prec': 'precision_macro',\n",
    "               'rec': 'recall_macro',\n",
    "               'f1': sklearn.metrics.make_scorer(sklearn.metrics.f1_score),\n",
    "               'roc_auc': sklearn.metrics.make_scorer(sklearn.metrics.roc_auc_score)}\n",
    "    rmse_cv_5 = cross_validate(model, np.array(list_posts_vec), np.array(list_personality_vec)[:,l], cv=5, scoring=scoring)\n",
    "    rmse_cv_10 = cross_validate(model,np.array(list_posts_vec), np.array(list_personality_vec)[:,l], cv=10, scoring=scoring)\n",
    "    print(\" RMSE test: {} RMSE cv5: {} RMSE cv10: {}\".format(rmse_val, rmse_cv_5, rmse_cv_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(type_indicators)):\n",
    "    print(\"\\n\\n{} ...\".format(type_indicators[l]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Posts in tf-idf representation\n",
    "\n",
    "# Let's train type indicator individually\n",
    "for l in range(len(type_indicators)):\n",
    "    print(\"\\n\\n{} ...\".format(type_indicators[l]))\n",
    "    \n",
    "    # Let's train type indicator individually\n",
    "    y_train_class = y_train[:,l]\n",
    "    y_test_class = y_test[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    model = svm.LinearSVR(C=10)\n",
    "\n",
    "    clf = model.fit(X_train, y_train_class)\n",
    "    \n",
    "#     pca = PCA(n_components=2).fit(X_train_seq_trunc)\n",
    "    \n",
    "#     pca_2d = pca.transform(X_train_seq_trunc)\n",
    "    \n",
    "#     svmClassifier_2d =   svm.LinearSVC(C=10,\n",
    "#                           class_weight='balanced').fit(   pca_2d, y_train_class)\n",
    "    \n",
    "#     for i in range(0, pca_2d.shape[0]):\n",
    "#         if y_train_res[i] == 0:\n",
    "#             c1 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='r', s=50,marker='+')\n",
    "#         elif y_train_res[i] == 1:\n",
    "#             c2 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='g',    s=50,marker='o')\n",
    "    \n",
    "#     pl.legend([c1, c2], [type_indicators[l][0], type_indicators[l][1]])\n",
    "#     x_min, x_max = pca_2d[:, 0].min() - 1,   pca_2d[:,0].max() + 1\n",
    "#     y_min, y_max = pca_2d[:, 1].min() - 1,   pca_2d[:, 1].max() + 1\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, .01),   np.arange(y_min, y_max, .01))\n",
    "#     Z = svmClassifier_2d.predict(np.c_[xx.ravel(),  yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     pl.contour(xx, yy, Z)\n",
    "#     pl.title('Support Vector Machine Decision Surface')\n",
    "#     pl.axis('off')\n",
    "#     pl.show()\n",
    "    \n",
    "    # make predictions  for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "#     accuracy = sklearn.metrics.accuracy_score(y_test, predictions)\n",
    "#     f1_score_measure = sklearn.metrics.f1_score(y_test, predictions)\n",
    "    rmse_val = rmse(y_test_class, predictions)\n",
    "    mae = sklearn.metrics.mean_absolute_error(y_test_class, predictions)\n",
    "    print(\" RMSE: {:.3f} MAE: {:.3f}\".format(rmse_val, mae))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
